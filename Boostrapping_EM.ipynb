{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate the data \n",
    "I'm aiming to generate 20000 datapoints from a Gaussian mixture model of k = 100 components. For simplicity, each datapoint is a scalar (1-dimension). \n",
    "To make it more realistic, I will random the weight of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20000 # Number of datapoints\n",
    "k = 100 # Number of components\n",
    "\n",
    "# Randomising the weights\n",
    "weights = np.random.random(size=(k, ))\n",
    "weights = weights / np.sum(weights) \n",
    "assert abs(np.sum(weights) - 1) < 0.00001\n",
    "\n",
    "# Decide which component we will sample from\n",
    "mixture_index = np.random.choice(k, size = n, replace = True, p = weights)\n",
    "assert mixture_index.shape == (n, )\n",
    "\n",
    "# Decide the random mean and variance of all 100 components\n",
    "mean = np.random.uniform(low=-20.0, high=20.0, size=(k, ))\n",
    "variance = np.random.uniform(low=0, high=3.0, size=(k, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using numpy for efficiency\n",
    "data = np.fromiter((np.random.normal(loc = mean[index], scale = np.sqrt([variance[index]])[0]) for index in mixture_index), float)\n",
    "assert data.shape == (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, bins=\"auto\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: The traditional EM algorithm\n",
    "The algorithm includes finding the probability of each point belonging to each component, then tune the mean and variance of each component according the the calculated probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a way to calculate pdf \n",
    "def pdf(x, mean, var):\n",
    "    assert var >= 0\n",
    "    return math.exp(-0.5 * (((float(x) - float(mean))) ** 2 ) / var) / (math.sqrt(2 * math.pi * var))\n",
    "\n",
    "# Test the function\n",
    "assert pdf(0, 0, 1) == 0.3989422804014327\n",
    "assert pdf(0.5, 0, 1) == 0.3520653267642995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27414015 0.23400901 0.36560499 0.3673932  0.51237975 0.40885679\n",
      " 0.72760286 0.40553564 0.8906912  0.08686441 0.49889843 0.49140418\n",
      " 0.17799974 0.43506238 0.0200561  0.04866437 0.42931182 0.29955814\n",
      " 0.03012795 0.17478946 0.21246087 0.80462723 0.44156527 0.35777876\n",
      " 0.87126827 0.78816174 0.87292638 0.81110704 0.04290283 0.87335029\n",
      " 0.99310686 0.93780679 0.96140308 0.39457965 0.24086012 0.78093334\n",
      " 0.85422282 0.30611113 0.63325345 0.96967908 0.78711162 0.57430174\n",
      " 0.09229893 0.78672782 0.42240235 0.30288648 0.87707859 0.4215961\n",
      " 0.9595822  0.11148371 0.05095562 0.98732811 0.67286103 0.44107044\n",
      " 0.81993555 0.21404758 0.13489028 0.3489893  0.30723238 0.73753973\n",
      " 0.28393179 0.50455115 0.50857222 0.32703714 0.02054651 0.7377169\n",
      " 0.31378317 0.31716955 0.2505203  0.29689135 0.31350865 0.02172159\n",
      " 0.30189098 0.9342512  0.47098591 0.24231545 0.24252971 0.28844763\n",
      " 0.5909655  0.23892156 0.47621666 0.00783082 0.41258015 0.37865945\n",
      " 0.5385957  0.87574337 0.57793106 0.07024516 0.03812031 0.23076517\n",
      " 0.36657962 0.94475419 0.38487336 0.15458791 0.12347021 0.5501088\n",
      " 0.00521821 0.34206065 0.32059651 0.74837892] [0.42757153 0.64074763 0.88636323 0.86528626 0.30312557 0.25246938\n",
      " 0.21838585 0.02672304 0.67448376 0.80527364 0.00446206 0.36459386\n",
      " 0.78699139 0.47706329 0.65802043 0.64210355 0.7801527  0.62152367\n",
      " 0.77313997 0.27602026 0.97545573 0.14407584 0.59183108 0.52949419\n",
      " 0.12586933 0.68203938 0.76947861 0.04762291 0.50851129 0.95660726\n",
      " 0.21864253 0.47290464 0.62388205 0.61584642 0.65593301 0.64653247\n",
      " 0.32593216 0.2186836  0.11762836 0.38323413 0.96768842 0.3834827\n",
      " 0.40217165 0.752962   0.93607824 0.84246708 0.8349413  0.26356172\n",
      " 0.47484117 0.42444952 0.19365876 0.01841538 0.12382537 0.32865958\n",
      " 0.60441183 0.11890966 0.87440777 0.79199404 0.22917566 0.12808175\n",
      " 0.81162837 0.79089372 0.39881665 0.76649155 0.11730081 0.50169864\n",
      " 0.937983   0.43923752 0.69861472 0.42446908 0.03908396 0.98328797\n",
      " 0.47173752 0.53341307 0.5022447  0.48152831 0.80461171 0.58091299\n",
      " 0.7321104  0.24490779 0.52105683 0.22186277 0.44108714 0.97232408\n",
      " 0.39268811 0.4414405  0.67216793 0.324925   0.59925697 0.18967124\n",
      " 0.01524103 0.07810673 0.73361327 0.45345446 0.70082217 0.27084474\n",
      " 0.4390062  0.64956266 0.23945119 0.0816356 ] [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "# This implementation is inspired by Dr. Martha White's notes for CMPUT367, page 107. \n",
    "def traditional_EM():\n",
    "    predicted_mean = np.random.random(k)\n",
    "    predicted_variance = np.random.random(k)\n",
    "    predicted_weights = np.ones(k) / k\n",
    "    \n",
    "    error = np.linalg.norm(mean - predicted_mean, 2)\n",
    "    error += np.linalg.norm(variance - predicted_variance, 2)\n",
    "    error += np.linalg.norm(weights - predicted_weights, 2)\n",
    "    \n",
    "    while error >= 8628370181.169346:\n",
    "        probability_matrix = np.zeros((n, k))\n",
    "        normalised_p_matrix = np.zeros((n, k))\n",
    "        for i in range(n):\n",
    "            denominator_sum = 0\n",
    "            for j in range(k):\n",
    "                probability_matrix[i, j] = predicted_weights[j] * pdf(data[i], predicted_mean[j], predicted_variance[j])\n",
    "                denominator_sum += probability_matrix[i, j]\n",
    "            probability_matrix[i, :] = probability_matrix[i, :] / denominator_sum\n",
    "         \n",
    "            normalised_p_matrix[i, :] = probability_matrix[i, :] / np.sum(probability_matrix[i, :])\n",
    "            \n",
    "        for j in range(k):\n",
    "            predicted_weights[j] = float(1/n) * np.sum(probability_matrix[:, j])\n",
    "            predicted_mean[j] = np.sum(normalised_p_matrix[:, j] * data)\n",
    "            predicted_variance[j] = np.sum(normalised_p_matrix[:, j] * (np.linalg.norm(data - predicted_mean[j], 2)) ** 2)\n",
    "        \n",
    "        error = np.linalg.norm(mean - predicted_mean, 2)\n",
    "        error += np.linalg.norm(variance - predicted_variance, 2)\n",
    "        error += np.linalg.norm(weights - predicted_weights, 2)\n",
    "        \n",
    "        print(\"error:\", error)\n",
    "    return predicted_mean, predicted_variance, predicted_weights\n",
    "\n",
    "\n",
    "predicted_mean, predicted_variance, predicted_weights = traditional_EM()\n",
    "print(predicted_mean, predicted_variance, predicted_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e145918ee62325bb783ae69933f229276b5458d89ab4d949d12da7376064d41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
