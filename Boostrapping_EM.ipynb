{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Generate the data \n",
    "I'm aiming to generate 20000 datapoints from a Gaussian mixture model of k = 100 components. For simplicity, each datapoint is a scalar (1-dimension). \n",
    "To make it more realistic, I will random the weight of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000 # Number of datapoints\n",
    "k = 20 # Number of components\n",
    "\n",
    "# Randomising the weights\n",
    "weights = np.random.random(size=(k, ))\n",
    "weights = weights / np.sum(weights) \n",
    "assert abs(np.sum(weights) - 1) < 0.0000001 # Ensuring a convex linear combination\n",
    "\n",
    "# Decide which component we will sample from\n",
    "mixture_index = np.random.choice(k, size = n, replace = True, p = weights)\n",
    "assert mixture_index.shape == (n, )\n",
    "\n",
    "# Decide the random mean and variance of all 100 components\n",
    "mean = np.random.uniform(low=-20.0, high=20.0, size=(k, ))\n",
    "variance = np.random.uniform(low=0, high=2.0, size=(k, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data using numpy for efficiency\n",
    "data = np.fromiter((np.random.normal(loc = mean[index], scale = np.sqrt([variance[index]])[0]) for index in mixture_index), float)\n",
    "assert data.shape == (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, bins=\"auto\", density=True)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: The traditional EM algorithm\n",
    "The algorithm includes finding the probability of each point belonging to each component, then tune the mean and variance of each component according the the calculated probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a way to calculate pdf \n",
    "def pdf(x, mean, var):\n",
    "    assert var >= 0\n",
    "    return math.exp(-0.5 * ((x - mean) ** 2 ) / var) / (math.sqrt(2 * math.pi * var))\n",
    "\n",
    "# Test the function\n",
    "assert pdf(0, 0, 1) == 0.3989422804014327\n",
    "assert pdf(0.5, 0, 1) == 0.3520653267642995\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metric to evaluate our return from EM alg.\n",
    "# KL divergence is an integral from -inf to +inf, in this case, I only use the n datapoints that we already generated.\n",
    "def KL_div(pred_mean, pred_var, pred_weights):\n",
    "    KL_divergence = 0\n",
    "    for i in range(n):\n",
    "        P_value = 0\n",
    "        Q_value = 0\n",
    "        for j in range(k):\n",
    "            Q_value += pred_weights[j] * pdf(data[i], pred_mean[j], pred_var[j])\n",
    "            P_value += weights[j] * pdf(data[i], mean[j], variance[j])\n",
    "        KL_divergence += P_value * math.log(P_value/Q_value, math.e) #- P_value + Q_value\n",
    "    \n",
    "    return KL_divergence\n",
    "assert KL_div(mean, variance, weights) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl div: 9028.01931858272\n",
      "kl div: 1076.469900107576\n",
      "kl div: 908.2022929756313\n",
      "kl div: 794.110635395626\n",
      "kl div: 690.0130556580931\n",
      "kl div: 586.4159156626114\n",
      "kl div: 499.2939653303597\n",
      "kl div: 436.1422710203223\n",
      "kl div: 394.1865467649598\n",
      "kl div: 368.05644799148376\n",
      "kl div: 350.13033626384845\n",
      "kl div: 335.59304323341615\n",
      "kl div: 322.184014934155\n",
      "kl div: 309.41692733725586\n",
      "kl div: 298.0005561727717\n",
      "kl div: 288.580560060142\n",
      "kl div: 280.34306945196954\n",
      "kl div: 270.50464786725337\n",
      "kl div: 254.56566048225943\n",
      "kl div: 230.6010952948011\n",
      "kl div: 204.8419692669276\n",
      "kl div: 183.52137583975315\n",
      "kl div: 168.1378821130281\n",
      "kl div: 157.69852502740122\n",
      "kl div: 150.67139164517351\n",
      "kl div: 145.84253311161305\n",
      "kl div: 142.4147438051885\n",
      "kl div: 139.89702923863737\n",
      "kl div: 137.9907957772682\n",
      "kl div: 136.51083121928724\n",
      "kl div: 135.3374287708743\n",
      "kl div: 134.38936116538042\n",
      "kl div: 133.60909637960933\n",
      "kl div: 132.95466695404056\n",
      "kl div: 132.39500689988094\n",
      "kl div: 131.90707149721285\n",
      "kl div: 131.47388288573865\n",
      "kl div: 131.08307107706204\n",
      "kl div: 130.72569443659057\n",
      "kl div: 130.39523019191236\n",
      "kl div: 130.08667477549812\n",
      "kl div: 129.79570862162242\n",
      "kl div: 129.5178703381697\n",
      "kl div: 129.24765391499702\n",
      "kl div: 128.97738560207648\n",
      "kl div: 128.69563816438142\n",
      "kl div: 128.3847607210024\n",
      "kl div: 128.01675845033787\n",
      "kl div: 127.54607113428115\n",
      "kl div: 126.89641822836845\n",
      "kl div: 125.93624772834839\n",
      "kl div: 124.4336311260892\n",
      "kl div: 121.98402523181045\n",
      "kl div: 117.95277433668045\n",
      "kl div: 111.67997388936931\n",
      "kl div: 103.53932725538105\n",
      "kl div: 96.52962027273863\n",
      "kl div: 93.97359801251284\n",
      "kl div: 94.02233961819047\n",
      "kl div: 94.2096537356389\n",
      "kl div: 94.23793222461764\n",
      "kl div: 94.20506063263454\n",
      "kl div: 94.15759061861294\n",
      "kl div: 94.10939745375462\n",
      "kl div: 94.06395665251576\n",
      "kl div: 94.02181903181884\n",
      "kl div: 93.98277927398598\n",
      "kl div: 93.94648405875671\n",
      "kl div: 93.91259228537292\n",
      "kl div: 93.8808083773976\n",
      "kl div: 93.85088204655315\n",
      "kl div: 93.82260118203575\n",
      "kl div: 93.79578481845162\n",
      "kl div: 93.77027750552199\n",
      "kl div: 93.74594497786995\n",
      "kl div: 93.72267078865166\n",
      "kl div: 93.70035362398457\n",
      "kl div: 93.67890510448053\n",
      "kl div: 93.65824794902852\n",
      "kl div: 93.63831442027274\n",
      "kl div: 93.619044997822\n",
      "kl div: 93.60038724092104\n",
      "kl div: 93.58229481164703\n",
      "kl div: 93.56472663559794\n",
      "kl div: 93.54764618089054\n",
      "kl div: 93.53102083917969\n",
      "kl div: 93.51482139464063\n",
      "kl div: 93.49902156870158\n",
      "kl div: 93.48359762988578\n",
      "kl div: 93.46852805954285\n",
      "kl div: 93.45379326547074\n",
      "kl div: 93.43937533648682\n",
      "kl div: 93.42525783200742\n",
      "kl div: 93.41142560147041\n",
      "kl div: 93.3978646292216\n",
      "kl div: 93.38456190109015\n",
      "kl div: 93.3715052894404\n",
      "kl div: 93.35868345394705\n",
      "kl div: 93.34608575577111\n",
      "kl div: 93.33370218316279\n",
      "kl div: 93.3215232867749\n",
      "kl div: 93.30954012330828\n",
      "kl div: 93.29774420622948\n",
      "kl div: 93.28612746259381\n",
      "kl div: 93.27468219504868\n",
      "kl div: 93.26340104833562\n",
      "kl div: 93.25227697964102\n",
      "kl div: 93.24130323227172\n",
      "kl div: 93.23047331224764\n",
      "kl div: 93.21978096738589\n",
      "kl div: 93.2092201685881\n",
      "kl div: 93.19878509306109\n",
      "kl div: 93.18847010923255\n",
      "kl div: 93.17826976318612\n",
      "kl div: 93.16817876640485\n",
      "kl div: 93.15819198475317\n",
      "kl div: 93.14830442851444\n",
      "kl div: 93.1385112434047\n",
      "kl div: 93.12880770248475\n",
      "kl div: 93.11918919888181\n",
      "kl div: 93.10965123925051\n",
      "kl div: 93.10018943792743\n",
      "kl div: 93.09079951173395\n",
      "kl div: 93.0814772753547\n",
      "kl div: 93.07221863729515\n",
      "kl div: 93.06301959635095\n",
      "kl div: 93.05387623857685\n",
      "kl div: 93.0447847347304\n",
      "kl div: 93.03574133813459\n",
      "kl div: 93.02674238299336\n",
      "kl div: 93.0177842830853\n",
      "kl div: 93.00886353085231\n",
      "kl div: 92.99997669684652\n",
      "kl div: 92.99112042952447\n",
      "kl div: 92.9822914553817\n",
      "kl div: 92.97348657939254\n",
      "kl div: 92.96470268576196\n",
      "kl div: 92.95593673895158\n",
      "kl div: 92.9471857850003\n",
      "kl div: 92.93844695305833\n",
      "kl div: 92.92971745721665\n",
      "kl div: 92.92099459850934\n",
      "kl div: 92.91227576716365\n",
      "kl div: 92.90355844500723\n",
      "kl div: 92.89484020807208\n",
      "kl div: 92.88611872934243\n",
      "kl div: 92.87739178162214\n",
      "kl div: 92.86865724053993\n",
      "kl div: 92.85991308761257\n",
      "kl div: 92.85115741338821\n",
      "kl div: 92.84238842063404\n",
      "kl div: 92.83360442752182\n",
      "kl div: 92.82480387080146\n",
      "kl div: 92.81598530894492\n",
      "kl div: 92.80714742519878\n",
      "kl div: 92.79828903054552\n",
      "kl div: 92.7894090665258\n",
      "kl div: 92.78050660789923\n",
      "kl div: 92.77158086510687\n",
      "kl div: 92.7626311865089\n",
      "kl div: 92.75365706035899\n",
      "kl div: 92.74465811650498\n",
      "kl div: 92.73563412776473\n",
      "kl div: 92.72658501095589\n",
      "kl div: 92.71751082758391\n",
      "kl div: 92.70841178410306\n",
      "kl div: 92.69928823181212\n",
      "kl div: 92.69014066629873\n",
      "kl div: 92.68096972644851\n",
      "kl div: 92.67177619301938\n",
      "kl div: 92.6625609867492\n",
      "kl div: 92.65332516601107\n",
      "kl div: 92.64406992401157\n",
      "kl div: 92.63479658554665\n",
      "kl div: 92.6255066032983\n",
      "kl div: 92.61620155372387\n",
      "kl div: 92.60688313251595\n",
      "kl div: 92.59755314967393\n",
      "kl div: 92.58821352421322\n",
      "kl div: 92.57886627851589\n",
      "kl div: 92.56951353237855\n",
      "kl div: 92.56015749676783\n",
      "kl div: 92.55080046732672\n",
      "kl div: 92.54144481764929\n",
      "kl div: 92.53209299239295\n",
      "kl div: 92.52274750021823\n",
      "kl div: 92.51341090664056\n",
      "kl div: 92.50408582677262\n",
      "kl div: 92.49477491806668\n",
      "kl div: 92.48548087301329\n",
      "kl div: 92.47620641187636\n",
      "kl div: 92.46695427550677\n",
      "kl div: 92.45772721821457\n",
      "kl div: 92.4485280007866\n",
      "kl div: 92.43935938363279\n",
      "kl div: 92.43022412010767\n",
      "kl div: 92.42112495002515\n",
      "kl div: 92.41206459338964\n",
      "kl div: 92.40304574435207\n",
      "kl div: 92.39407106541609\n",
      "kl div: 92.3851431819128\n",
      "kl div: 92.37626467673225\n",
      "kl div: 92.36743808535489\n",
      "kl div: 92.35866589116011\n",
      "kl div: 92.34995052103216\n",
      "kl div: 92.3412943412613\n",
      "kl div: 92.33269965375891\n",
      "kl div: 92.32416869255373\n",
      "kl div: 92.31570362059273\n",
      "kl div: 92.30730652685133\n",
      "kl div: 92.2989794237109\n",
      "kl div: 92.29072424464863\n",
      "kl div: 92.28254284219547\n",
      "kl div: 92.27443698616662\n",
      "kl div: 92.26640836217128\n",
      "kl div: 92.25845857036518\n",
      "kl div: 92.25058912447662\n",
      "kl div: 92.2428014510496\n",
      "kl div: 92.23509688893331\n",
      "kl div: 92.22747668899635\n",
      "kl div: 92.21994201404615\n",
      "kl div: 92.2124939389591\n",
      "kl div: 92.2051334509891\n",
      "kl div: 92.19786145028156\n",
      "kl div: 92.19067875054779\n",
      "kl div: 92.1835860798884\n",
      "kl div: 92.17658408180526\n",
      "kl div: 92.16967331631696\n",
      "kl div: 92.16285426123446\n",
      "kl div: 92.15612731355134\n",
      "kl div: 92.14949279094874\n",
      "kl div: 92.14295093339885\n",
      "kl div: 92.13650190488444\n",
      "kl div: 92.13014579517666\n",
      "kl div: 92.12388262171937\n",
      "kl div: 92.11771233156342\n",
      "kl div: 92.11163480337555\n",
      "kl div: 92.10564984949798\n",
      "kl div: 92.09975721804868\n",
      "kl div: 92.09395659507265\n",
      "kl div: 92.08824760671614\n",
      "kl div: 92.08262982142922\n",
      "kl div: 92.07710275219279\n",
      "kl div: 92.07166585874587\n",
      "kl div: 92.06631854983455\n",
      "kl div: 92.06106018545299\n",
      "kl div: 92.05589007907894\n",
      "kl div: 92.0508074999093\n",
      "kl div: 92.04581167507439\n",
      "kl div: 92.04090179183115\n",
      "kl div: 92.03607699974698\n",
      "kl div: 92.03133641283557\n",
      "kl div: 92.02667911168892\n",
      "kl div: 92.02210414554828\n",
      "kl div: 92.0176105343578\n",
      "kl div: 92.01319727077797\n",
      "kl div: 92.00886332213761\n",
      "kl div: 92.00460763237254\n",
      "kl div: 92.00042912388193\n",
      "kl div: 91.9963266993728\n",
      "kl div: 91.99229924362496\n",
      "kl div: 91.98834562522434\n",
      "kl div: 91.98446469823179\n",
      "kl div: 91.98065530380849\n",
      "kl div: 91.97691627177754\n",
      "kl div: 91.97324642214531\n",
      "kl div: 91.96964456655182\n",
      "kl div: 91.96610950967786\n",
      "kl div: 91.96264005059977\n",
      "kl div: 91.95923498407488\n",
      "kl div: 91.95589310178522\n",
      "kl div: 91.95261319353516\n",
      "kl div: 91.94939404837177\n",
      "kl div: 91.94623445567328\n",
      "kl div: 91.94313320618363\n",
      "kl div: 91.94008909298434\n",
      "kl div: 91.93710091243678\n",
      "kl div: 91.93416746505241\n",
      "kl div: 91.93128755633349\n",
      "kl div: 91.92845999756662\n",
      "kl div: 91.9256836065556\n",
      "kl div: 91.92295720832911\n",
      "kl div: 91.92027963579665\n",
      "kl div: 91.91764973036733\n",
      "kl div: 91.91506634252109\n",
      "kl div: 91.91252833234962\n",
      "kl div: 91.91003457006492\n",
      "kl div: 91.90758393645123\n",
      "kl div: 91.90517532331137\n",
      "kl div: 91.90280763385263\n",
      "kl div: 91.9004797830611\n",
      "kl div: 91.89819069803744\n",
      "kl div: 91.89593931830376\n",
      "kl div: 91.89372459608471\n",
      "kl div: 91.89154549655558\n",
      "kl div: 91.88940099808548\n",
      "kl div: 91.88729009242226\n",
      "kl div: 91.8852117848878\n",
      "kl div: 91.8831650945359\n",
      "kl div: 91.88114905428598\n",
      "kl div: 91.87916271105203\n",
      "kl div: 91.87720512583479\n",
      "kl div: 91.87527537381095\n",
      "kl div: 91.87337254440072\n",
      "kl div: 91.87149574131581\n",
      "kl div: 91.86964408259941\n",
      "kl div: 91.86781670065108\n",
      "kl div: 91.86601274222966\n",
      "kl div: 91.86423136845592\n",
      "kl div: 91.86247175480385\n",
      "kl div: 91.86073309105839\n",
      "kl div: 91.85901458129972\n",
      "kl div: 91.8573154438459\n",
      "kl div: 91.85563491120973\n",
      "kl div: 91.85397223001732\n",
      "kl div: 91.85232666095956\n",
      "kl div: 91.85069747870538\n",
      "kl div: 91.84908397181327\n",
      "kl div: 91.84748544264582\n",
      "kl div: 91.84590120727084\n",
      "kl div: 91.84433059535878\n",
      "kl div: 91.84277295007601\n",
      "kl div: 91.84122762797298\n",
      "kl div: 91.83969399885662\n",
      "kl div: 91.83817144568974\n",
      "kl div: 91.83665936444655\n",
      "kl div: 91.8351571639916\n",
      "kl div: 91.83366426594934\n",
      "kl div: 91.83218010456042\n",
      "kl div: 91.8307041265585\n",
      "kl div: 91.82923579101305\n",
      "kl div: 91.82777456919759\n",
      "kl div: 91.82631994443663\n",
      "kl div: 91.82487141196675\n",
      "kl div: 91.82342847877632\n",
      "kl div: 91.82199066346912\n",
      "kl div: 91.82055749609621\n",
      "kl div: 91.81912851801496\n",
      "kl div: 91.81770328172531\n",
      "kl div: 91.81628135072269\n",
      "kl div: 91.81486229933287\n",
      "kl div: 91.81344571255622\n",
      "kl div: 91.81203118591314\n",
      "kl div: 91.81061832528381\n",
      "kl div: 91.80920674674596\n",
      "kl div: 91.80779607641871\n",
      "kl div: 91.80638595029997\n",
      "kl div: 91.80497601411082\n",
      "kl div: 91.8035659231314\n",
      "kl div: 91.8021553420404\n",
      "kl div: 91.80074394476176\n",
      "kl div: 91.79933141429593\n",
      "kl div: 91.79791744256912\n",
      "kl div: 91.79650173026869\n",
      "kl div: 91.79508398669036\n",
      "kl div: 91.7936639295759\n",
      "kl div: 91.79224128495729\n",
      "kl div: 91.79081578700273\n",
      "kl div: 91.78938717785783\n",
      "kl div: 91.78795520749739\n",
      "kl div: 91.78651963356074\n",
      "kl div: 91.78508022121042\n",
      "kl div: 91.78363674297673\n",
      "kl div: 91.782188978604\n",
      "kl div: 91.78073671490746\n",
      "kl div: 91.77927974562024\n",
      "kl div: 91.7778178712485\n",
      "kl div: 91.77635089893134\n",
      "kl div: 91.7748786422854\n",
      "kl div: 91.773400921278\n",
      "kl div: 91.77191756207306\n",
      "kl div: 91.77042839689969\n",
      "kl div: 91.76893326391183\n",
      "kl div: 91.76743200705465\n",
      "kl div: 91.76592447593045\n",
      "kl div: 91.76441052566354\n",
      "kl div: 91.7628900167745\n",
      "kl div: 91.76136281504311\n",
      "kl div: 91.75982879139815\n",
      "kl div: 91.75828782177304\n",
      "kl div: 91.75673978699986\n",
      "kl div: 91.75518457267594\n",
      "kl div: 91.75362206905632\n",
      "kl div: 91.75205217093033\n",
      "kl div: 91.75047477750572\n",
      "kl div: 91.74888979230116\n",
      "kl div: 91.7472971230381\n",
      "kl div: 91.74569668152411\n",
      "kl div: 91.74408838355369\n",
      "kl div: 91.74247214880218\n",
      "kl div: 91.74084790072702\n",
      "kl div: 91.73921556646411\n",
      "kl div: 91.73757507673385\n",
      "kl div: 91.73592636574622\n",
      "kl div: 91.73426937110986\n",
      "kl div: 91.73260403373597\n",
      "kl div: 91.7309302977579\n",
      "kl div: 91.72924811043936\n",
      "kl div: 91.7275574220988\n",
      "kl div: 91.72585818601902\n",
      "kl div: 91.72415035837388\n",
      "kl div: 91.72243389815219\n",
      "kl div: 91.72070876708253\n",
      "kl div: 91.71897492956286\n",
      "kl div: 91.71723235258648\n",
      "kl div: 91.71548100567922\n",
      "kl div: 91.71372086083767\n",
      "kl div: 91.71195189245485\n",
      "kl div: 91.71017407727801\n",
      "kl div: 91.70838739432968\n",
      "kl div: 91.70659182487091\n",
      "kl div: 91.70478735233563\n",
      "kl div: 91.70297396228322\n",
      "kl div: 91.70115164235445\n",
      "kl div: 91.69932038221319\n",
      "kl div: 91.69748017351714\n",
      "kl div: 91.69563100986552\n",
      "kl div: 91.69377288675926\n",
      "kl div: 91.69190580156632\n",
      "kl div: 91.69002975348893\n",
      "kl div: 91.68814474352425\n",
      "kl div: 91.68625077443954\n",
      "kl div: 91.68434785073677\n",
      "kl div: 91.68243597863295\n",
      "kl div: 91.68051516602594\n",
      "kl div: 91.67858542248783\n",
      "kl div: 91.67664675922848\n",
      "kl div: 91.6746991890836\n",
      "kl div: 91.67274272650418\n",
      "kl div: 91.67077738753518\n",
      "kl div: 91.6688031898075\n",
      "kl div: 91.66682015252789\n",
      "kl div: 91.66482829646652\n",
      "kl div: 91.66282764396686\n",
      "kl div: 91.66081821891832\n",
      "kl div: 91.65880004677626\n",
      "kl div: 91.65677315455167\n",
      "kl div: 91.65473757081749\n",
      "kl div: 91.65269332570945\n",
      "kl div: 91.65064045093759\n",
      "kl div: 91.64857897979135\n",
      "kl div: 91.64650894714939\n",
      "kl div: 91.6444303894978\n",
      "kl div: 91.64234334493403\n",
      "kl div: 91.6402478531918\n",
      "kl div: 91.63814395565466\n",
      "kl div: 91.63603169538051\n",
      "kl div: 91.63391111711371\n",
      "kl div: 91.63178226731641\n",
      "kl div: 91.62964519419207\n",
      "kl div: 91.62749994770759\n",
      "kl div: 91.62534657962814\n",
      "kl div: 91.62318514353824\n",
      "kl div: 91.62101569488266\n",
      "kl div: 91.61883829098977\n",
      "kl div: 91.61665299111758\n",
      "kl div: 91.61445985647197\n",
      "kl div: 91.61225895026024\n",
      "kl div: 91.6100503377209\n",
      "kl div: 91.60783408616221\n",
      "kl div: 91.60561026500748\n",
      "kl div: 91.60337894582922\n",
      "kl div: 91.6011402023999\n",
      "kl div: 91.59889411072795\n",
      "kl div: 91.5966407491059\n",
      "kl div: 91.59438019815119\n",
      "kl div: 91.5921125408535\n",
      "kl div: 91.58983786262137\n",
      "kl div: 91.58755625132825\n",
      "kl div: 91.58526779735054\n",
      "kl div: 91.58297259362598\n",
      "kl div: 91.58067073568927\n",
      "kl div: 91.5783623217262\n",
      "kl div: 91.5760474526117\n",
      "kl div: 91.5737262319563\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tutha\\Desktop\\Personal Project\\Boostrapping-Expectation-Maximization-Algorithm\\Boostrapping_EM.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkl div:\u001b[39m\u001b[39m\"\u001b[39m, KL_divergence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_mean, predicted_variance, predicted_weights\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predicted_mean, predicted_variance, predicted_weights \u001b[39m=\u001b[39m traditional_EM()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_mean, predicted_variance, predicted_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(mean, variance, weights)\n",
      "\u001b[1;32mc:\\Users\\tutha\\Desktop\\Personal Project\\Boostrapping-Expectation-Maximization-Algorithm\\Boostrapping_EM.ipynb Cell 9\u001b[0m in \u001b[0;36mtraditional_EM\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39massert\u001b[39;00m predicted_variance\u001b[39m.\u001b[39mall() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mabs\u001b[39m(np\u001b[39m.\u001b[39msum(predicted_weights) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m \u001b[39m0.0000001\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     KL_divergence \u001b[39m=\u001b[39m KL_div(predicted_mean, predicted_variance, predicted_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkl div:\u001b[39m\u001b[39m\"\u001b[39m, KL_divergence)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m predicted_mean, predicted_variance, predicted_weights\n",
      "\u001b[1;32mc:\\Users\\tutha\\Desktop\\Personal Project\\Boostrapping-Expectation-Maximization-Algorithm\\Boostrapping_EM.ipynb Cell 9\u001b[0m in \u001b[0;36mKL_div\u001b[1;34m(pred_mean, pred_var, pred_weights)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m Q_value \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(k):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Q_value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred_weights[j] \u001b[39m*\u001b[39m pdf(data[i], pred_mean[j], pred_var[j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     P_value \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m weights[j] \u001b[39m*\u001b[39m pdf(data[i], mean[j], variance[j])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m KL_divergence \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m P_value \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog(P_value\u001b[39m/\u001b[39mQ_value, math\u001b[39m.\u001b[39me) \u001b[39m#- P_value + Q_value\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\tutha\\Desktop\\Personal Project\\Boostrapping-Expectation-Maximization-Algorithm\\Boostrapping_EM.ipynb Cell 9\u001b[0m in \u001b[0;36mpdf\u001b[1;34m(x, mean, var)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpdf\u001b[39m(x, mean, var):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39massert\u001b[39;00m var \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tutha/Desktop/Personal%20Project/Boostrapping-Expectation-Maximization-Algorithm/Boostrapping_EM.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m ((x \u001b[39m-\u001b[39m mean) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m ) \u001b[39m/\u001b[39m var) \u001b[39m/\u001b[39m (math\u001b[39m.\u001b[39;49msqrt(\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m math\u001b[39m.\u001b[39;49mpi \u001b[39m*\u001b[39;49m var))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This implementation is based on Dr. Martha White's notes for CMPUT367, page 107. \n",
    "def traditional_EM():\n",
    "    predicted_mean = np.random.uniform(low=-5, high=5, size=(k, ))\n",
    "    predicted_variance = np.random.uniform(low=0, high=5, size=(k, ))\n",
    "    predicted_weights = np.random.random(size=(k, ))\n",
    "    predicted_weights = predicted_weights / np.sum(predicted_weights) \n",
    "    \n",
    "    KL_divergence = KL_div(predicted_mean, predicted_variance, predicted_weights)\n",
    "    print(\"kl div:\", KL_divergence)\n",
    "    \n",
    "    while KL_divergence > 35:\n",
    "        \n",
    "        probability_matrix = np.zeros((n, k))\n",
    "        normalised_p_matrix = np.zeros((n, k))\n",
    "        for i in range(n):\n",
    "            denominator_sum = 0\n",
    "            for j in range(k):\n",
    "                probability_matrix[i, j] = predicted_weights[j] * pdf(data[i], predicted_mean[j], predicted_variance[j])\n",
    "                denominator_sum += probability_matrix[i, j]\n",
    "            probability_matrix[i, :] = probability_matrix[i, :] / denominator_sum\n",
    "        \n",
    "        \n",
    "        for j in range(k):\n",
    "            normalised_p_matrix[:, j] = probability_matrix[:, j] / np.sum(probability_matrix[:, j])\n",
    "                \n",
    "        #print(predicted_mean)\n",
    "        for j in range(k):\n",
    "            predicted_weights[j] = (1/n) * np.sum(probability_matrix[:, j])\n",
    "            predicted_mean[j] = np.sum(normalised_p_matrix[:, j] * data)\n",
    "            \n",
    "            predicted_variance[j] = np.sum(normalised_p_matrix[:, j] * (data - predicted_mean[j]) ** 2)\n",
    "            \n",
    "        assert predicted_variance.all() > 0\n",
    "        assert abs(np.sum(predicted_weights) - 1) < 0.0000001\n",
    "        \n",
    "        KL_divergence = KL_div(predicted_mean, predicted_variance, predicted_weights)\n",
    "        print(\"kl div:\", KL_divergence)\n",
    "        \n",
    "    return predicted_mean, predicted_variance, predicted_weights\n",
    "\n",
    "\n",
    "predicted_mean, predicted_variance, predicted_weights = traditional_EM()\n",
    "print(predicted_mean, predicted_variance, predicted_weights)\n",
    "print(mean, variance, weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e145918ee62325bb783ae69933f229276b5458d89ab4d949d12da7376064d41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
